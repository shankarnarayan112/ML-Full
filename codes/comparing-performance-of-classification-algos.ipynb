{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1 align=\"center\">Assignment</h1>\n<h3 align=\"center\">Faisal Akhtar</h3>\n<h3 align=\"center\">Roll No.: 17/1409</h3>\n<p>Machine Learning - B.Sc. Hons Computer Science - VIth Semester</p>\n<p>Compare performance of some classification algorithms like logistic regression and neural networks on Iris dataset and Seeds dataset (available on UCI).</p>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from sklearn import datasets\nimport pandas as pd","execution_count":4,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"iris = datasets.load_iris(return_X_y=False)","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create numeric classes for species (0,1,2) \niris.loc[iris['Name']=='virginica','species']=0\niris.loc[iris['Name']=='versicolor','species']=1\niris.loc[iris['Name']=='setosa','species'] = 2\niris = iris[iris['species']!=2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create Input and Output columns\nX = iris[['PetalLength', 'PetalWidth']].values.T\nY = iris[['species']].values.T\nY = Y.astype('uint8'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Make a scatter plot\nplt.scatter(X[0, :], X[1, :], c=Y[0,:], s=40, cmap=plt.cm.Spectral);\nplt.title(\"IRIS DATA | Blue - Versicolor, Red - Virginica \")\nplt.xlabel('Petal Length')\nplt.ylabel('Petal Width')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def initialize_parameters(n_x, n_h, n_y):\n    \n    np.random.seed(2) # we set up a seed so that our output matches ours although the initialization is random.\n    \n    W1 = np.random.randn(n_h, n_x) * 0.01 #weight matrix of shape (n_h, n_x)\n    b1 = np.zeros(shape=(n_h, 1))  #bias vector of shape (n_h, 1)\n    W2 = np.random.randn(n_y, n_h) * 0.01   #weight matrix of shape (n_y, n_h)\n    b2 = np.zeros(shape=(n_y, 1))  #bias vector of shape (n_y, 1)\n       \n    #store parameters into a dictionary    \n    parameters = {\"W1\": W1,\n                  \"b1\": b1,\n                  \"W2\": W2,\n                  \"b2\": b2}\n    \n    return parameters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Function to define the size of the layer\ndef layer_sizes(X, Y):\n    n_x = X.shape[0] # size of input layer\n    n_h = 6# size of hidden layer\n    n_y = Y.shape[0] # size of","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#retrieve intialized parameters from dictionary    \ndef forward_propagation(X, parameters):\n    W1 = parameters['W1']\n    b1 = parameters['b1']\n    W2 = parameters['W2']\n    b2 = parameters['b2']\n    \n    \n    # Implement Forward Propagation to calculate A2 (probability)\n    Z1 = np.dot(W1, X) + b1\n    A1 = np.tanh(Z1)  #tanh activation function\n    Z2 = np.dot(W2, A1) + b2\n    A2 = 1/(1+np.exp(-Z2))  #sigmoid activation function\n    \n    cache = {\"Z1\": Z1,\n             \"A1\": A1,\n             \"Z2\": Z2,\n             \"A2\": A2}\n    \n    return A2, cache","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_cost(A2, Y, parameters):   \n    m = Y.shape[1] # number of training examples\n    \n    # Retrieve W1 and W2 from parameters\n    W1 = parameters['W1']\n    W2 = parameters['W2']\n    \n    # Compute the cross-entropy cost\n    logprobs = np.multiply(np.log(A2), Y) + np.multiply((1 - Y), np.log(1 - A2))\n    cost = - np.sum(logprobs) / m\n    \n    return cost","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def backward_propagation(parameters, cache, X, Y):\n# Number of training examples\n    m = X.shape[1]\n    \n    # First, retrieve W1 and W2 from the dictionary \"parameters\".\nW1 = parameters['W1']\n    W2 = parameters['W2']\n    ### END CODE HERE ###\n        \n    # Retrieve A1 and A2 from dictionary \"cache\".\n    A1 = cache['A1']\n    A2 = cache['A2']\n    \n    # Backward propagation: calculate dW1, db1, dW2, db2. \n    dZ2= A2 - Y\n    dW2 = (1 / m) * np.dot(dZ2, A1.T)\n    db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)\n    dZ1 = np.multiply(np.dot(W2.T, dZ2), 1 - np.power(A1, 2))\n    dW1 = (1 / m) * np.dot(dZ1, X.T)\n    db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)\n    grads = {\"dW1\": dW1,\n             \"db1\": db1,\n             \"dW2\": dW2,\n             \"db2\": db2}\n    \n    return grads","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def update_parameters(parameters, grads, learning_rate=1.2):\n# Retrieve each parameter from the dictionary \"parameters\"\nW1 = parameters['W1']\n    b1 = parameters['b1']\n    W2 = parameters['W2']\n    b2 = parameters['b2']\n    \n    # Retrieve each gradient from the dictionary \"grads\"\n    dW1 = grads['dW1']\n    db1 = grads['db1']\n    dW2 = grads['dW2']\n    db2 = grads['db2']\n    \n    # Update rule for each parameter\n    W1 = W1 - learning_rate * dW1\n    b1 = b1 - learning_rate * db1\n    W2 = W2 - learning_rate * dW2\n    b2 = b2 - learning_rate * db2\n    \n    parameters = {\"W1\": W1,\n                  \"b1\": b1,\n                  \"W2\": W2,\n                  \"b2\": b2}\n    \n    return parameters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def nn_model(X, Y, n_h, num_iterations=10000, print_cost=False):\n    np.random.seed(3)\n    n_x = layer_sizes(X, Y)[0]\n    n_y = layer_sizes(X, Y)[2]\n    \n    # Initialize parameters, then retrieve W1, b1, W2, b2. Inputs: \"n_x, n_h, n_y\". Outputs = \"W1, b1, W2, b2, parameters\".\n    parameters = initialize_parameters(n_x, n_h, n_y)\n    W1 = parameters['W1']\n    b1 = parameters['b1']\n    W2 = parameters['W2']\n    b2 = parameters['b2']\n    \n    # Loop (gradient descent)\n    for i in range(0, num_iterations):\n        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n        A2, cache = forward_propagation(X, parameters)\n        # Cost function. Inputs: \"A2, Y, parameters\". Outputs: \"cost\".\n        cost = compute_cost(A2, Y, parameters)\n        # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n        grads = backward_propagation(parameters, cache, X, Y)\n        # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n        parameters = update_parameters(parameters, grads)\n        # Print the cost every 1000 iterations\n        if print_cost and i % 1000 == 0:\n            print (\"Cost after iteration %i: %f\" % (i, cost))\n        return parameters,n_h","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parameters = nn_model(X,Y , n_h = 6, num_iterations=10000, print_cost=True)\n\ndef plot_decision_boundary(model, X, y):\n    # Set min and max values and give it some padding\n    x_min, x_max = X[0, :].min() - 0.25, X[0, :].max() + 0.25\n    y_min, y_max = X[1, :].min() - 0.25, X[1, :].max() + 0.25\n    h = 0.01\n    # Generate a grid of points with distance h between them\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    # Predict the function value for the whole grid\n    Z = model(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    # Plot the contour and training examples\n    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n    plt.ylabel('x2')\n    plt.xlabel('x1')\n    plt.scatter(X[0, :], X[1, :], c=y, cmap=plt.cm.Spectral)\n    plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y[0,:])\n    plt.title(\"Decision Boundary for hidden layer size \" + str(6))\n    plt.xlabel('Petal Length')\n    plt.ylabel('Petal Width') ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}